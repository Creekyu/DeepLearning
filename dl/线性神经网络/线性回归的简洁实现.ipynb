{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features,labels = d2l.synthetic_data(true_w,true_b,1000)\n",
    "# 生成数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    # *args表示不定长参数\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    # 将特征和结果打包成一个pytorch内部的dataset数据结构\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "    # DataLoader从上面的dataset中随机选取指定数量样本集，返回值也是一个DataLoader数据结构\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size) # 原理和之前实现的迭代器一样\n",
    "# 通过iter()转换为一个迭代器再可通过next提取其中每一项\n",
    "# 通过迭代器每一次都可以获取一个batch_size大小的数据集（可以无限获取）\n",
    "# for i in range(1000000000):\n",
    "#     print(next(iter(data_iter)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,1)) # 单层神经网络模型输入2维输出1维\n",
    "# 放入Sequential容器中（存放神经网络的层）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0,0.01) # 指定初始w参数\n",
    "net[0].bias.data.fill_(0) # 指定初试b参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "loss = nn.MSELoss() # 损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(),lr=0.03) # 直接调用优化函数\n",
    "# net.parameters()提取出所有参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1,loss0.000103\n",
      "epoch2,loss0.000103\n",
      "epoch3,loss0.000103\n",
      "epoch4,loss0.000103\n",
      "epoch5,loss0.000103\n",
      "epoch6,loss0.000104\n",
      "epoch7,loss0.000104\n",
      "epoch8,loss0.000104\n",
      "epoch9,loss0.000104\n",
      "epoch10,loss0.000106\n",
      "epoch11,loss0.000105\n",
      "epoch12,loss0.000106\n",
      "epoch13,loss0.000105\n",
      "epoch14,loss0.000105\n",
      "epoch15,loss0.000104\n",
      "epoch16,loss0.000104\n",
      "epoch17,loss0.000104\n",
      "epoch18,loss0.000105\n",
      "epoch19,loss0.000105\n",
      "epoch20,loss0.000105\n",
      "epoch21,loss0.000104\n",
      "epoch22,loss0.000105\n",
      "epoch23,loss0.000105\n",
      "epoch24,loss0.000105\n",
      "epoch25,loss0.000105\n",
      "epoch26,loss0.000105\n",
      "epoch27,loss0.000105\n",
      "epoch28,loss0.000105\n",
      "epoch29,loss0.000105\n",
      "epoch30,loss0.000105\n",
      "epoch31,loss0.000104\n",
      "epoch32,loss0.000105\n",
      "epoch33,loss0.000105\n",
      "epoch34,loss0.000105\n",
      "epoch35,loss0.000104\n",
      "epoch36,loss0.000104\n",
      "epoch37,loss0.000103\n",
      "epoch38,loss0.000104\n",
      "epoch39,loss0.000104\n",
      "epoch40,loss0.000104\n",
      "epoch41,loss0.000104\n",
      "epoch42,loss0.000105\n",
      "epoch43,loss0.000104\n",
      "epoch44,loss0.000105\n",
      "epoch45,loss0.000105\n",
      "epoch46,loss0.000104\n",
      "epoch47,loss0.000103\n",
      "epoch48,loss0.000104\n",
      "epoch49,loss0.000104\n",
      "epoch50,loss0.000104\n",
      "epoch51,loss0.000104\n",
      "epoch52,loss0.000104\n",
      "epoch53,loss0.000104\n",
      "epoch54,loss0.000104\n",
      "epoch55,loss0.000105\n",
      "epoch56,loss0.000104\n",
      "epoch57,loss0.000104\n",
      "epoch58,loss0.000104\n",
      "epoch59,loss0.000104\n",
      "epoch60,loss0.000104\n",
      "epoch61,loss0.000103\n",
      "epoch62,loss0.000103\n",
      "epoch63,loss0.000103\n",
      "epoch64,loss0.000103\n",
      "epoch65,loss0.000103\n",
      "epoch66,loss0.000104\n",
      "epoch67,loss0.000103\n",
      "epoch68,loss0.000103\n",
      "epoch69,loss0.000103\n",
      "epoch70,loss0.000103\n",
      "epoch71,loss0.000103\n",
      "epoch72,loss0.000104\n",
      "epoch73,loss0.000104\n",
      "epoch74,loss0.000104\n",
      "epoch75,loss0.000103\n",
      "epoch76,loss0.000103\n",
      "epoch77,loss0.000103\n",
      "epoch78,loss0.000103\n",
      "epoch79,loss0.000103\n",
      "epoch80,loss0.000103\n",
      "epoch81,loss0.000104\n",
      "epoch82,loss0.000104\n",
      "epoch83,loss0.000103\n",
      "epoch84,loss0.000104\n",
      "epoch85,loss0.000103\n",
      "epoch86,loss0.000104\n",
      "epoch87,loss0.000104\n",
      "epoch88,loss0.000104\n",
      "epoch89,loss0.000104\n",
      "epoch90,loss0.000104\n",
      "epoch91,loss0.000104\n",
      "epoch92,loss0.000103\n",
      "epoch93,loss0.000103\n",
      "epoch94,loss0.000104\n",
      "epoch95,loss0.000104\n",
      "epoch96,loss0.000104\n",
      "epoch97,loss0.000104\n",
      "epoch98,loss0.000104\n",
      "epoch99,loss0.000104\n",
      "epoch100,loss0.000104\n",
      "epoch101,loss0.000104\n",
      "epoch102,loss0.000104\n",
      "epoch103,loss0.000104\n",
      "epoch104,loss0.000104\n",
      "epoch105,loss0.000105\n",
      "epoch106,loss0.000103\n",
      "epoch107,loss0.000103\n",
      "epoch108,loss0.000103\n",
      "epoch109,loss0.000103\n",
      "epoch110,loss0.000103\n",
      "epoch111,loss0.000103\n",
      "epoch112,loss0.000103\n",
      "epoch113,loss0.000103\n",
      "epoch114,loss0.000104\n",
      "epoch115,loss0.000104\n",
      "epoch116,loss0.000103\n",
      "epoch117,loss0.000103\n",
      "epoch118,loss0.000103\n",
      "epoch119,loss0.000103\n",
      "epoch120,loss0.000103\n",
      "epoch121,loss0.000103\n",
      "epoch122,loss0.000103\n",
      "epoch123,loss0.000103\n",
      "epoch124,loss0.000103\n",
      "epoch125,loss0.000104\n",
      "epoch126,loss0.000104\n",
      "epoch127,loss0.000104\n",
      "epoch128,loss0.000104\n",
      "epoch129,loss0.000105\n",
      "epoch130,loss0.000106\n",
      "epoch131,loss0.000106\n",
      "epoch132,loss0.000106\n",
      "epoch133,loss0.000107\n",
      "epoch134,loss0.000107\n",
      "epoch135,loss0.000107\n",
      "epoch136,loss0.000107\n",
      "epoch137,loss0.000107\n",
      "epoch138,loss0.000105\n",
      "epoch139,loss0.000106\n",
      "epoch140,loss0.000107\n",
      "epoch141,loss0.000106\n",
      "epoch142,loss0.000104\n",
      "epoch143,loss0.000104\n",
      "epoch144,loss0.000104\n",
      "epoch145,loss0.000103\n",
      "epoch146,loss0.000103\n",
      "epoch147,loss0.000103\n",
      "epoch148,loss0.000103\n",
      "epoch149,loss0.000103\n",
      "epoch150,loss0.000103\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    # time = 0\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y) # 由于参数loss函数自带所以不需要写\n",
    "        trainer.zero_grad()\n",
    "        l.backward() # 损失自动求和故不用sum()\n",
    "        trainer.step() # 更新参数\n",
    "        break # 不加break每次大循环都进行100次更新\n",
    "        # time += 1\n",
    "        # print(time)\n",
    "    l = loss(net(features),labels)\n",
    "    print(f'epoch{epoch+1},loss{l:f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}